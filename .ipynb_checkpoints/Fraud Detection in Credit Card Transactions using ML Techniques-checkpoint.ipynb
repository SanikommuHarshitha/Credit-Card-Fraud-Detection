{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraud Detection in Credit Card Transactions using ML Techniques\n",
    "*Oladimeji Salau*  \n",
    "https://github.com/dimtics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "The objective in this project is to build machine learning models to classify or identify fraudulent card transactions from a given card transactions data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud/data) contains two-days credit card transactions made in September 2013 by European cardholders. The dataset is highly unbalanced with a low percentage of fraudulent transactions within several records of normal transactions. The positive class (frauds) account for 0.172% (492 frauds out of 284,807 transactions) of all transactions.\n",
    "\n",
    "Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset.  Feature 'Class' is the target variable with value 1 in case of fraud and 0 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plot_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3a4d49d243ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# plot functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplot_functions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# scikit packages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plot_functions'"
     ]
    }
   ],
   "source": [
    "# Import basic libraries \n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from imblearn.over_sampling import ADASYN \n",
    "from collections import Counter\n",
    "import seaborn as sn\n",
    "\n",
    "# plot functions\n",
    "import plot_functions as pf\n",
    "\n",
    "# scikit packages\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "from sklearn import metrics\n",
    "\n",
    "# settings\n",
    "%matplotlib inline\n",
    "sn.set_style(\"dark\")\n",
    "sn.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data  \n",
    "The dataset used in this project is freely available at: https://www.kaggle.com/mlg-ulb/creditcardfraud/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View top 5 records\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the number of records in the dataset\n",
    "print('The dataset contains {0} rows and {1} columns.'.format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values and data types of the columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore label class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Normal transactions count: ', df['Class'].value_counts().values[0])\n",
    "print('Fraudulent transactions count: ', df['Class'].value_counts().values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate feature data (predictors) from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature data (predictors)\n",
    "X = df.iloc[:, :-1]\n",
    "\n",
    "# label class\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize data\n",
    "Scale the data to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sampling  \n",
    "As noted earlier, the dataset is unbalanced. Training unbalanced dataset with learning algorithms may lead to misclassification of minority class. Therefore, to compensate for the unbalancedness, we will use ADASYN oversampling method as implemented in imbalanced-learn package to resample the dataset.  \n",
    "ADASYN (ADAptive SYNthetic) is an [oversampling technique](https://www.datasciencecentral.com/profiles/blogs/handling-imbalanced-data-sets-in-supervised-learning-using-family) that adaptively generates minority data samples according to their distributions using K nearest neighbor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the ADASYN over-sampling\n",
    "ada = ADASYN(random_state=42)\n",
    "print('Original dataset shape {}'.format(Counter(y_train)))\n",
    "X_res, y_res = ada.fit_sample(X_train, y_train)\n",
    "print('Resampled dataset shape {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models  \n",
    "Three machine learning algorithms: Logistic Regression, Naive Baye, and RandomForest classifiers were trained using the processed feature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_res, y_res \n",
    "\n",
    "# Train LogisticRegression Model\n",
    "LGR_Classifier = LogisticRegression()\n",
    "LGR_Classifier.fit(X_train, y_train);\n",
    "\n",
    "# Train Decision Tree Model\n",
    "RDF_Classifier = RandomForestClassifier(random_state=0)\n",
    "RDF_Classifier.fit(X_train, y_train);\n",
    "\n",
    "# Train Bernoulli Naive Baye Model\n",
    "BNB_Classifier = BernoulliNB()\n",
    "BNB_Classifier.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Models  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "modlist = [('RandomForest Classifier', RDF_Classifier),('LogisticRegression', LGR_Classifier),\n",
    "('Naive Baiye Classifier', BNB_Classifier)] \n",
    "\n",
    "models = [j for j in modlist]\n",
    "\n",
    "print()\n",
    "print('========================== Model Evaluation Results ========================' \"\\n\")  \n",
    "\n",
    "for i, v in models:\n",
    "    scores = cross_val_score(v, X_train, y_train, cv=10)\n",
    "    accuracy = metrics.accuracy_score(y_train, v.predict(X_train))\n",
    "    confusion_matrix = metrics.confusion_matrix(y_train, v.predict(X_train))\n",
    "    classification = metrics.classification_report(y_train, v.predict(X_train))\n",
    "    print('===== {} ====='.format(i))\n",
    "    print()\n",
    "    print (\"Cross Validation Mean Score: \", '{}%'.format(np.round(scores.mean(), 3) * 100))  \n",
    "    print() \n",
    "    print (\"Model Accuracy: \", '{}%'.format(np.round(accuracy, 3) * 100)) \n",
    "    print()\n",
    "    print(\"Confusion Matrix:\" \"\\n\", confusion_matrix)\n",
    "    print()\n",
    "    print(\"Classification Report:\" \"\\n\", classification) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test models\n",
    "classdict = {'normal':0, 'fraudulent':1}\n",
    "print()\n",
    "print('========================== Model Test Results ========================' \"\\n\")   \n",
    "\n",
    "for i, v in models:\n",
    "    accuracy = metrics.accuracy_score(y_test, v.predict(X_test))\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, v.predict(X_test))\n",
    "    classification = metrics.classification_report(y_test, v.predict(X_test))   \n",
    "    print('=== {} ==='.format(i))\n",
    "    print (\"Model Accuracy: \",  '{}%'.format(np.round(accuracy, 3) * 100))\n",
    "    print()\n",
    "    print(\"Confusion Matrix:\" \"\\n\", confusion_matrix)\n",
    "    print()\n",
    "    pf.plot_confusion_matrix(confusion_matrix, classes = list(classdict.keys()), title='Confusion Matrix Plot', cmap=plt.cm.summer)\n",
    "    print() \n",
    "    print(\"Classification Report:\" \"\\n\", classification) \n",
    "    print() \n",
    "\n",
    "print('============================= ROC Curve ===============================' \"\\n\")      \n",
    "pf.plot_roc_auc(arg1=models, arg2=X_test, arg3=y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
